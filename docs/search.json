[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Villard",
    "section": "",
    "text": "Maybe you will like this. Villard manages your data science pipelines. Split a big project into smaller discrete steps for a maintainable and reproducible workflow. Perhaps you don’t even need to your existing code too much."
  },
  {
    "objectID": "iris_setup.html",
    "href": "iris_setup.html",
    "title": "Iris classifier project",
    "section": "",
    "text": "In this project, we will learn:"
  },
  {
    "objectID": "iris_setup.html#data-catalog",
    "href": "iris_setup.html#data-catalog",
    "title": "Iris classifier project",
    "section": "Data catalog",
    "text": "Data catalog\nHard coding the data paths is not a good idea. Villard provides a data catalog to help you manage your data and to make the tracking of data easier. It defines everything related to data input and output, as well as its meta-data, inside the configuration file.\nWe will start to make use of data catalog for this example. At minimum, you need to define path and type for each entry in the data catalog. For all supported data types, you can refer to data types. Data placement follows cookiecutter recommendations.\n// We'll start using variables, one of Jsonnet's features\nlocal model_params = {\n    \"max_depth\": 5,\n    \"min_samples_leaf\": 1,\n    \"min_samples_split\": 2,\n    \"n_estimators\": 100,\n    \"random_state\": 42\n};\n\n{\n    // Data catalog section\n    \"data_catalog\": {\n        \"iris_raw\": {\n            \"path\": \"data/01_raw/iris.csv\",\n            \"type\": \"DT_PANDAS_DATAFRAME\"\n        },\n        \"trained_model\": {\n            \"path\": \"data/03_output/trained_model.pkl\",\n            \"type\": \"DT_PICKLE\"\n        }\n    },\n\n    // The modules containing data engineering and data science \n    // function implementations.\n    \"node_implementation_modules\": [\n        \"data_engineering\", \"data_science\"\n    ],\n\n    // The pipeline definitions (i.e., node interdependencies and its parameters)\n    \"pipeline_definition\": {\n        \"data_engineering\": {\n            \"split_data\": {\n                \"train_size\": 0.8,\n                \"random_state\": 42\n            },\n            \"preprocess_data\": {\n                \"path\": \"data::iris_raw\"\n            }, \n        },\n\n        \"data_science\": {\n            \"train_model\": {\n                \"data\": \"ref::preprocess_data\",\n                \"model_params\": model_params,\n            },\n            \"predict\": {\n                \"data\": \"ref::preprocess_data\",\n                \"model\": \"ref::train_model\"\n            }, \n            \"evaluate_model\": {\n                \"actual_labels\": \"ref::preprocess_data\",\n                \"predicted_labels\": \"ref::predict\"\n            }\n        },\n\n        // Jsonnet feature to concatenate two objects.\n        // In this context, the _default pipeline is the end-to-end steps \n        // from data engineering to data science.\n        \"_default\": self.data_engineering + self.data_science,\n    }\n}\n\n\n\n\n\n\nTip\n\n\n\nYou can even split the Jsonnet file into separate files and import them into the main file. For example, you can put the data_catalog entries into data_catalog.libsonnet:\n{\n    \"iris_raw\": {\n        \"path\": \"data/01_raw/iris.csv\",\n        \"type\": \"DT_PANDAS_DATAFRAME\"\n    },\n    \"trained_model\": {\n        \"path\": \"data/03_output/trained_model.pkl\",\n        \"type\": \"DT_PICKLE\"\n    }\n}\nthen import it in the main configuration file as follows:\n{\n    \"data_catalog\": import data_catalog.libsonnet,\n    ...\n}"
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "pip install git+https://github.com/ariaghora/villard"
  },
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Getting started",
    "section": "",
    "text": "Hello world pipeline\nFor our first example, let’s consider the following simple pipeline:\n\n\n\n\nflowchart LR\n    get_name --> greet;\n\n\n\n\n\n\n\n\nVillard is all about pipeline orchestration. It is designed to cope with frequent pipeline changes. All pipeline definitions and changes are made in configuration files. Let’s create one. In your project folder, create a config.yml file. At very least, a configuration file must contain a pipeline_definition section, and that section must contain _default key.\nnode_implementation_modules:\n    - implementation\n\npipeline_definition:\n    _default:\n        combine_name:\n            first_name: \"John\"\n            last_name: \"Doe\"\n        greet:\n            name: \"ref::combine_name\"\nIt also accommodates multiple pipelines, so you may add more definitions other than _default.\nThe combine_name node will take the values of first_name and last_name. This node will output a string that is the concatenation of both names. The output of combine_name is then passed to greet node to be displayed. Note that the name value of greet node is ref::combine_name. It means that the greet node refers to the output of combine_name node (which is the concatenation names).\nThen we make the pipeline nodes’ implementation in implementation.py.\nfrom villard import V\n\n@V.node(\"combine_name\")\ndef combine_name(first_name, last_name):\n    return first_name + \" \" + last_name\n\n@V.node(\"greet\")\ndef greet(name):\n    print (\"Hello, \" + name + \"!\")\nNow execute the pipeline by invoking following command:\n$ villard run config.yml"
  }
]