[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Villard",
    "section": "",
    "text": "Maybe you will like this. Villard manages your data science pipelines. Split a big project into smaller discrete steps for a maintainable and reproducible workflow. Perhaps you don’t even need to your existing code too much."
  },
  {
    "objectID": "iris_setup.html",
    "href": "iris_setup.html",
    "title": "Iris classifier project",
    "section": "",
    "text": "In this project, we will learn:"
  },
  {
    "objectID": "iris_setup.html#data-catalog",
    "href": "iris_setup.html#data-catalog",
    "title": "Iris classifier project",
    "section": "Data catalog",
    "text": "Data catalog\nHard coding the data paths is not a good idea. Villard provides a data catalog to help you manage your data and to make the tracking of data easier. It defines everything related to data input and output, as well as its meta-data, inside the configuration file.\nWe will start to make use of data catalog for this example. At minimum, you need to define path and type for each entry in the data catalog. For all supported data types, you can refer to data types. Data placement follows cookiecutter recommendations.\n/**\n Step params\n */\nlocal preprocess_data_params = {\n  data: 'data::local_iris',\n};\n\nlocal split_data_params = {\n  data: 'ref::preprocess_data',\n  train_frac: 0.8,\n};\n\nlocal train_model_params = {\n  data: 'ref::split_data',\n  model_class: 'obj::SVC',\n  model_params: {\n    kernel: 'rbf',\n    C: 10,\n  },\n};\n\nlocal make_inference_params = {\n  // by default, the model is loaded from a file\n  // defined in data catalog\n  model: 'data::trained_model',\n  feature_df: 'data::test_features',\n  stdout: false,\n};\n\nlocal evaluate_model_params = {\n  predicted_target_df: 'ref::make_inference',\n  actual_target_df: 'data::test_target',\n};\n\n\n/**\n Main configuration entry point\n */\n{\n  data_catalog: {\n    local_iris: {\n      path: 'data/01_raw/iris.csv',\n      type: 'DT_PANDAS_DATAFRAME',\n      write_params: {\n        index: false,\n      },\n    },\n    trained_model: {\n      path: 'data/03_output/model.pkl',\n      type: 'DT_PICKLE',\n      track_on_write: true,\n    },\n  },\n\n  step_implementation_modules: ['steps.data_engineering', 'steps.data_science'],\n\n  experiment_output_dir: 'experiment_output',\n\n  pipeline_definition: {\n    _default: self.data_engineering_pipeline + self.training_pipeline,\n\n    data_engineering_pipeline: {\n      preprocess_data: preprocess_data_params,\n      split_data: split_data_params,\n    },\n\n    training_pipeline: {\n      train_model: train_model_params,\n      // Inference using freshly trained model instead of the loaded model.\n      make_inference: make_inference_params { model: 'ref::train_model' },\n      evaluate_model: evaluate_model_params,\n    },\n\n    inference_pipeline: {\n      // Inference using loaded model (default action)\n      make_inference: make_inference_params { stdout: true },\n    },\n  },\n}\n\n\n\n\n\n\nTip\n\n\n\nYou can even split the Jsonnet file into separate files and import them into the main file. For example, you can put the data_catalog entries into data_catalog.libsonnet:\n{\n    \"iris_raw\": {\n        \"path\": \"data/01_raw/iris.csv\",\n        \"type\": \"DT_PANDAS_DATAFRAME\"\n    },\n    \"trained_model\": {\n        \"path\": \"data/03_output/trained_model.pkl\",\n        \"type\": \"DT_PICKLE\"\n    }\n}\nthen import it in the main configuration file as follows:\n{\n    \"data_catalog\": import \"data_catalog.libsonnet\",\n    ...\n}"
  },
  {
    "objectID": "iris_setup.html#the-data-engineering-code-stepsdata_engineering.py",
    "href": "iris_setup.html#the-data-engineering-code-stepsdata_engineering.py",
    "title": "Iris classifier project",
    "section": "The data engineering code steps/data_engineering.py",
    "text": "The data engineering code steps/data_engineering.py\nfrom typing import Tuple\nimport pandas as pd\nfrom villard import pipeline\nfrom sklearn.model_selection import train_test_split\n\n\n@pipeline.step(\"preprocess_data\")\ndef load_data(data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    df = data\n    X = df.iloc[:, :-1]\n    y = df.iloc[:, -1]\n    pipeline.track(\"trained_model\", X)\n\n    return X, y\n\n\n@pipeline.step(\"split_data\")\ndef split_data(\n    data: Tuple[pd.DataFrame, pd.DataFrame], train_frac: float\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    X, y = data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_frac)\n\n    pipeline.write_data(\"test_features\", X_test)\n    pipeline.write_data(\"test_target\", y_test)\n    return X_train, X_test, y_train, y_test"
  },
  {
    "objectID": "iris_setup.html#the-data-science-code-stepsdata_science.py",
    "href": "iris_setup.html#the-data-science-code-stepsdata_science.py",
    "title": "Iris classifier project",
    "section": "The data science code steps/data_science.py",
    "text": "The data science code steps/data_science.py\nfrom typing import Any, Dict, Tuple\n\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nfrom villard import pipeline\n\nfrom sklearn import svm, tree, linear_model\n\npipeline.register_object(\"SVC\", svm.SVC)\npipeline.register_object(\"DecisionTreeClassifier\", tree.DecisionTreeClassifier)\npipeline.register_object(\"LogisticRegression\", linear_model.LogisticRegression)\n\n\n@pipeline.step(\"train_model\")\ndef train_model(\n    data: Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame],\n    model_class: Any,\n    model_params: Dict[str, Any],\n) -> Any:\n    X_train, _, y_train, _ = data\n    clf = model_class(**model_params).fit(X_train, y_train)\n\n    pipeline.write_data(\"trained_model\", clf)\n    pipeline.track(\"trained_model\", clf.__class__.__name__)\n    for k, v in model_params.items():\n        pipeline.track(k, v)\n    return clf\n\n\n@pipeline.step(\"make_inference\")\ndef make_inference(model: Any, feature_df: pd.DataFrame, stdout: bool) -> pd.DataFrame:\n    pred = model.predict(feature_df)\n    if stdout:\n        print(pred)\n    return pred\n\n\n@pipeline.step(\"evaluate_model\")\ndef evaluate_model(\n    actual_target_df: pd.DataFrame, predicted_target_df: pd.DataFrame\n) -> Any:\n    accuracy = accuracy_score(actual_target_df, predicted_target_df)\n    print(\"Accuracy: \", accuracy)\n    pipeline.track(\"test_accuracy\", accuracy)\n    return accuracy"
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "pip install git+https://github.com/ariaghora/villard"
  },
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Getting started",
    "section": "",
    "text": "Hello world pipeline\nFor our first example, let’s consider the following simple pipeline:\n\n\n\n\nflowchart LR\n    get_name --> greet;\n\n\n\n\n\n\n\n\nVillard is all about pipeline orchestration. It is designed to cope with frequent pipeline changes. All pipeline definitions and changes are made in configuration files. Let’s create one. In your project folder, create a config.yml file.\n\n\n\n\n\n\nNote\n\n\n\nVillard supports not only YAML, but also JSON and Jsonnet.\n\n\nAt very least, a configuration file must contain a pipeline_definition section, and that section must contain _default key.\nstep_implementation_modules:\n    - implementation\n\npipeline_definition:\n    _default:\n        combine_name:\n            first_name: \"John\"\n            last_name: \"Doe\"\n        greet:\n            name: \"ref::combine_name\"\nIt also accommodates multiple pipelines, so you may add more definitions other than _default.\nThe combine_name step will take the values of first_name and last_name. This step will output a string that is the concatenation of both names. The output of combine_name is then passed to greet step to be displayed. Note that the name value of greet step is ref::combine_name. It means that the greet step refers to the output of combine_name step (which is the concatenation names).\nThen we make the pipeline steps’ implementation in implementation.py.\nfrom villard import pipeline\n\n@pipeline.step(\"combine_name\")\ndef combine_name(first_name, last_name):\n    return first_name + \" \" + last_name\n\n@pipeline.step(\"greet\")\ndef greet(name):\n    print (\"Hello, \" + name + \"!\")\nNow execute the pipeline by invoking following command:\n$ villard run config.yml"
  }
]